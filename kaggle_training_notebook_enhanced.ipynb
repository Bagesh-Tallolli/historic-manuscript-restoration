{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c914ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# KAGGLE TRAINING NOTEBOOK - Complete Restoration Pipeline\n",
    "# ============================================================================\n",
    "# Upload this to Kaggle as a notebook\n",
    "# Enable GPU: Settings â†’ GPU T4 x2\n",
    "# Run All Cells\n",
    "# ============================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acbdfe1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "get_ipython().run_cell_magic('capture', '', '!pip install einops lpips roboflow opencv-python-headless scikit-image matplotlib -q')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab3aa50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from pathlib import Path\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "from einops import rearrange\n",
    "from einops.layers.torch import Rearrange\n",
    "import lpips\n",
    "from skimage.metrics import peak_signal_noise_ratio, structural_similarity\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(f\"âœ“ PyTorch: {torch.__version__}\")\n",
    "print(f\"âœ“ CUDA: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"âœ“ GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd277a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "from roboflow import Roboflow\n",
    "\n",
    "API_KEY = \"EBJvHlgSWLyW1Ir6ctkH\"\n",
    "WORKSPACE = \"neeew\"\n",
    "PROJECT = \"yoyoyo-mptyx-ijqfp\"\n",
    "VERSION = 1\n",
    "DATASET_LOC = \"/kaggle/working/dataset\"\n",
    "\n",
    "print(\"ðŸ“¥ Downloading dataset from Roboflow...\")\n",
    "rf = Roboflow(api_key=API_KEY)\n",
    "proj = rf.workspace(WORKSPACE).project(PROJECT)\n",
    "dataset = proj.version(VERSION).download(\"folder\", location=DATASET_LOC)\n",
    "\n",
    "TRAIN_DIR = f\"{DATASET_LOC}/train\"\n",
    "VAL_DIR = f\"{DATASET_LOC}/valid\" if os.path.exists(f\"{DATASET_LOC}/valid\") else None\n",
    "\n",
    "print(f\"âœ… Dataset downloaded to: {DATASET_LOC}\")\n",
    "print(f\"Train: {len(list(Path(TRAIN_DIR).glob('**/*.*')))} files\")\n",
    "if VAL_DIR:\n",
    "    print(f\"Valid: {len(list(Path(VAL_DIR).glob('**/*.*')))} files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f454c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "class PatchEmbed(nn.Module):\n",
    "    def __init__(self, img_size=256, patch_size=16, in_c=3, embed_dim=768):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Sequential(\n",
    "            nn.Conv2d(in_c, embed_dim, kernel_size=patch_size, stride=patch_size),\n",
    "            Rearrange('b c h w -> b (h w) c')\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.proj(x)\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim=768, heads=12, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.heads = heads\n",
    "        self.scale = (dim // heads) ** -0.5\n",
    "        self.qkv = nn.Linear(dim, dim * 3)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, N, C = x.shape\n",
    "        qkv = self.qkv(x).reshape(B, N, 3, self.heads, C // self.heads).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
    "        return self.proj(self.drop(x))\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, dim=768, hidden=3072, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(dim, hidden)\n",
    "        self.fc2 = nn.Linear(hidden, dim)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.drop(self.fc2(self.drop(F.gelu(self.fc1(x)))))\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, dim=768, heads=12, mlp_ratio=4.0, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(dim)\n",
    "        self.attn = Attention(dim, heads, dropout)\n",
    "        self.norm2 = nn.LayerNorm(dim)\n",
    "        self.mlp = MLP(dim, int(dim * mlp_ratio), dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.norm1(x))\n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "        return x\n",
    "\n",
    "class ViTRestorer(nn.Module):\n",
    "    \"\"\"\n",
    "    Vision Transformer for Image Restoration\n",
    "    With skip connections for better detail preservation\n",
    "    \"\"\"\n",
    "    def __init__(self, img_size=256, patch_size=16, in_c=3, out_c=3,\n",
    "                 embed_dim=768, depth=12, heads=12, mlp_ratio=4.0, use_skip=True):\n",
    "        super().__init__()\n",
    "        self.patch_embed = PatchEmbed(img_size, patch_size, in_c, embed_dim)\n",
    "        num_patches = (img_size // patch_size) ** 2\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches, embed_dim))\n",
    "        self.blocks = nn.ModuleList([Block(embed_dim, heads, mlp_ratio) for _ in range(depth)])\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "\n",
    "        # Decoder head\n",
    "        self.head = nn.Linear(embed_dim, patch_size * patch_size * out_c)\n",
    "\n",
    "        # Skip connection fusion (preserves fine details from input)\n",
    "        self.use_skip = use_skip\n",
    "        if use_skip:\n",
    "            self.skip_fusion = nn.Conv2d(in_c + out_c, out_c, kernel_size=1)\n",
    "            nn.init.kaiming_normal_(self.skip_fusion.weight)\n",
    "            nn.init.constant_(self.skip_fusion.bias, 0)\n",
    "\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        nn.init.trunc_normal_(self.pos_embed, std=0.02)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Store input for skip connection\n",
    "        input_img = x\n",
    "\n",
    "        # Encode: patches + positional embedding\n",
    "        x = self.patch_embed(x) + self.pos_embed\n",
    "\n",
    "        # Transform: apply transformer blocks\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        x = self.norm(x)\n",
    "\n",
    "        # Decode: reconstruct image from patches\n",
    "        x = self.head(x)\n",
    "        h = w = self.img_size // self.patch_size\n",
    "        x = x.reshape(x.shape[0], h, w, self.patch_size, self.patch_size, 3)\n",
    "        x = x.permute(0, 5, 1, 3, 2, 4).reshape(x.shape[0], 3, self.img_size, self.img_size)\n",
    "\n",
    "        # Apply skip connection (helps preserve input details and textures)\n",
    "        if self.use_skip:\n",
    "            x = torch.cat([x, input_img], dim=1)  # Concatenate restored + input\n",
    "            x = self.skip_fusion(x)  # Fuse them intelligently\n",
    "\n",
    "        return x\n",
    "\n",
    "print(\"âœ“ Model architecture defined (with skip connections for better restoration)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297282c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "class CombinedLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Combined loss for better restoration:\n",
    "    - L1 loss for pixel-level accuracy\n",
    "    - Perceptual loss for visual quality\n",
    "    \"\"\"\n",
    "    def __init__(self, l1_weight=1.0, perceptual_weight=0.1):\n",
    "        super().__init__()\n",
    "        self.l1_weight = l1_weight\n",
    "        self.perceptual_weight = perceptual_weight\n",
    "\n",
    "        # Perceptual loss using LPIPS\n",
    "        try:\n",
    "            self.lpips_fn = lpips.LPIPS(net='vgg').eval()\n",
    "            for param in self.lpips_fn.parameters():\n",
    "                param.requires_grad = False\n",
    "            self.use_perceptual = True\n",
    "            print(\"âœ“ Using L1 + Perceptual loss\")\n",
    "        except:\n",
    "            self.use_perceptual = False\n",
    "            print(\"âš ï¸ Perceptual loss not available, using L1 only\")\n",
    "\n",
    "    def forward(self, pred, target, device='cuda'):\n",
    "        # L1 loss\n",
    "        l1_loss = F.l1_loss(pred, target)\n",
    "\n",
    "        # Perceptual loss\n",
    "        if self.use_perceptual and self.perceptual_weight > 0:\n",
    "            try:\n",
    "                self.lpips_fn = self.lpips_fn.to(device)\n",
    "                # LPIPS expects inputs in [-1, 1] range\n",
    "                pred_scaled = pred * 2 - 1\n",
    "                target_scaled = target * 2 - 1\n",
    "                perceptual_loss = self.lpips_fn(pred_scaled, target_scaled).mean()\n",
    "\n",
    "                total_loss = self.l1_weight * l1_loss + self.perceptual_weight * perceptual_loss\n",
    "                return total_loss\n",
    "            except:\n",
    "                return l1_loss\n",
    "        else:\n",
    "            return l1_loss\n",
    "\n",
    "print(\"âœ“ Loss function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee76f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "class ManuscriptDataset(Dataset):\n",
    "    def __init__(self, data_dir, img_size=256, mode='train', augment=True):\n",
    "        self.data_dir = Path(data_dir)\n",
    "        self.img_size = img_size\n",
    "        self.mode = mode\n",
    "        self.augment = augment and mode == 'train'\n",
    "\n",
    "        exts = ['.jpg', '.jpeg', '.png', '.tif', '.tiff', '.bmp']\n",
    "        self.images = []\n",
    "        for ext in exts:\n",
    "            self.images.extend(list(self.data_dir.glob(f'**/*{ext}')))\n",
    "            self.images.extend(list(self.data_dir.glob(f'**/*{ext.upper()}')))\n",
    "        self.images = sorted(set(self.images))\n",
    "        print(f\"{mode}: {len(self.images)} images\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def _degrade(self, img):\n",
    "        \"\"\"Apply realistic manuscript degradation\"\"\"\n",
    "        img = img.astype(np.float32) / 255.0\n",
    "\n",
    "        # Additive noise (simulates scanning noise, paper texture)\n",
    "        if random.random() > 0.2:\n",
    "            noise_level = random.uniform(0.02, 0.10)\n",
    "            img += np.random.normal(0, noise_level, img.shape)\n",
    "\n",
    "        # Gaussian blur (simulates focus issues, age blur)\n",
    "        if random.random() > 0.2:\n",
    "            kernel_size = random.choice([3, 5, 7])\n",
    "            img = cv2.GaussianBlur(img, (kernel_size, kernel_size), 0)\n",
    "\n",
    "        # Contrast/brightness degradation (faded ink)\n",
    "        if random.random() > 0.3:\n",
    "            contrast = random.uniform(0.5, 0.9)\n",
    "            brightness = random.uniform(0.05, 0.20)\n",
    "            img = contrast * img + brightness\n",
    "\n",
    "        # Aging tint (yellowing paper)\n",
    "        if random.random() > 0.3:\n",
    "            tint = np.array([1.0, random.uniform(0.90, 0.98), random.uniform(0.75, 0.90)])\n",
    "            img *= tint\n",
    "\n",
    "        # JPEG compression artifacts (simulates poor digitization)\n",
    "        if random.random() > 0.5:\n",
    "            img_uint8 = (np.clip(img, 0, 1) * 255).astype(np.uint8)\n",
    "            quality = random.randint(60, 90)\n",
    "            encode_param = [int(cv2.IMWRITE_JPEG_QUALITY), quality]\n",
    "            _, enc = cv2.imencode('.jpg', img_uint8, encode_param)\n",
    "            img = cv2.imdecode(enc, 1).astype(np.float32) / 255.0\n",
    "\n",
    "        return np.clip(img, 0, 1)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.images[idx]\n",
    "        img = cv2.imread(str(img_path))\n",
    "        if img is None:\n",
    "            img = np.array(Image.open(img_path).convert('RGB'))\n",
    "        else:\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Clean target\n",
    "        clean = img.astype(np.float32) / 255.0\n",
    "\n",
    "        # Degraded input\n",
    "        degraded = self._degrade(img.copy())\n",
    "\n",
    "        # Data augmentation\n",
    "        if self.augment:\n",
    "            if random.random() > 0.5:\n",
    "                degraded = np.fliplr(degraded).copy()\n",
    "                clean = np.fliplr(clean).copy()\n",
    "            if random.random() > 0.5:\n",
    "                angle = random.choice([90, 180, 270])\n",
    "                degraded = np.rot90(degraded, k=angle//90).copy()\n",
    "                clean = np.rot90(clean, k=angle//90).copy()\n",
    "\n",
    "        # Resize\n",
    "        degraded = cv2.resize(degraded, (self.img_size, self.img_size))\n",
    "        clean = cv2.resize(clean, (self.img_size, self.img_size))\n",
    "\n",
    "        # To tensor\n",
    "        degraded = torch.from_numpy(degraded).permute(2, 0, 1).float()\n",
    "        clean = torch.from_numpy(clean).permute(2, 0, 1).float()\n",
    "\n",
    "        return {'degraded': degraded, 'clean': clean}\n",
    "\n",
    "print(\"âœ“ Enhanced dataset with realistic degradation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6cabe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "IMG_SIZE = 256\n",
    "BATCH_SIZE = 16\n",
    "NUM_EPOCHS = 100\n",
    "LEARNING_RATE = 1e-4\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(f\"Device: {DEVICE}\")\n",
    "print(f\"Image size: {IMG_SIZE}\")\n",
    "print(f\"Batch size: {BATCH_SIZE}\")\n",
    "print(f\"Epochs: {NUM_EPOCHS}\")\n",
    "print(f\"Learning rate: {LEARNING_RATE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1778854b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "train_dataset = ManuscriptDataset(TRAIN_DIR, IMG_SIZE, 'train', True)\n",
    "train_loader = DataLoader(train_dataset, BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)\n",
    "\n",
    "val_loader = None\n",
    "if VAL_DIR:\n",
    "    val_dataset = ManuscriptDataset(VAL_DIR, IMG_SIZE, 'val', False)\n",
    "    val_loader = DataLoader(val_dataset, BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "print(f\"Train batches: {len(train_loader)}\")\n",
    "if val_loader:\n",
    "    print(f\"Val batches: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c639f482",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "model = ViTRestorer(\n",
    "    img_size=IMG_SIZE,\n",
    "    embed_dim=768,\n",
    "    depth=12,\n",
    "    heads=12,\n",
    "    use_skip=True  # Enable skip connections!\n",
    ").to(DEVICE)\n",
    "\n",
    "criterion = CombinedLoss(l1_weight=1.0, perceptual_weight=0.1)\n",
    "criterion = criterion.to(DEVICE)\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=0.01)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, NUM_EPOCHS, eta_min=1e-6)\n",
    "\n",
    "os.makedirs('/kaggle/working/checkpoints', exist_ok=True)\n",
    "os.makedirs('/kaggle/working/test_results', exist_ok=True)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"âœ“ Model: {total_params:,} parameters\")\n",
    "print(f\"âœ“ Skip connections: ENABLED (for better detail preservation)\")\n",
    "\n",
    "# Continue in next message..."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
