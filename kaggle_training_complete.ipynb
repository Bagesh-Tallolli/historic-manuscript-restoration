{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca4a0288",
   "metadata": {},
   "source": [
    "# üèõÔ∏è Historic Manuscript Restoration - Complete Training Pipeline\n",
    "\n",
    "## üìã What This Notebook Does\n",
    "Trains a Vision Transformer (ViT) model to restore degraded Sanskrit manuscripts using:\n",
    "- **Synthetic Degradation**: Creates realistic degraded/clean pairs automatically\n",
    "- **Skip Connections**: Preserves fine details during restoration\n",
    "- **Perceptual Loss**: Improves visual quality beyond pixel-level metrics\n",
    "- **Automatic Dataset Download**: From Roboflow (hardcoded credentials)\n",
    "\n",
    "## ‚öôÔ∏è Quick Start\n",
    "1. **Enable GPU**: Settings ‚Üí Accelerator ‚Üí GPU T4 x2\n",
    "2. **Click \"Run All\"**: Everything is automated\n",
    "3. **Wait ~5 hours**: Training completes automatically\n",
    "4. **Download Models**: `best_psnr.pth` and `final.pth` from Output panel\n",
    "\n",
    "## üéØ Key Features\n",
    "‚úÖ Paired training: Each image ‚Üí generates degraded version ‚Üí model learns to restore  \n",
    "‚úÖ Automatic data augmentation (flip, rotate)  \n",
    "‚úÖ Validation metrics (PSNR, SSIM)  \n",
    "‚úÖ Sample visualizations saved  \n",
    "‚úÖ Ready for download and local use  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44869232",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install einops lpips roboflow opencv-python-headless scikit-image matplotlib -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a54364",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from pathlib import Path\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "from einops import rearrange\n",
    "from einops.layers.torch import Rearrange\n",
    "import lpips\n",
    "from skimage.metrics import peak_signal_noise_ratio, structural_similarity\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(f\"‚úì PyTorch: {torch.__version__}\")\n",
    "print(f\"‚úì CUDA: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"‚úì GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"‚úì GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c2290bb",
   "metadata": {},
   "source": [
    "## üì• Step 1: Download Dataset (Automatic)\n",
    "\n",
    "The dataset will be downloaded automatically from Roboflow with hardcoded credentials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802edd3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from roboflow import Roboflow\n",
    "\n",
    "# Hardcoded Roboflow API credentials\n",
    "API_KEY = \"EBJvHlgSWLyW1Ir6ctkH\"\n",
    "WORKSPACE = \"neeew\"\n",
    "PROJECT = \"yoyoyo-mptyx-ijqfp\"\n",
    "VERSION = 1\n",
    "DATASET_LOC = \"/kaggle/working/dataset\"\n",
    "\n",
    "print(\"üì• Downloading dataset from Roboflow...\")\n",
    "print(f\"   Workspace: {WORKSPACE}\")\n",
    "print(f\"   Project: {PROJECT}\")\n",
    "print(f\"   Version: {VERSION}\")\n",
    "\n",
    "rf = Roboflow(api_key=API_KEY)\n",
    "proj = rf.workspace(WORKSPACE).project(PROJECT)\n",
    "dataset = proj.version(VERSION).download(\"folder\", location=DATASET_LOC)\n",
    "\n",
    "TRAIN_DIR = f\"{DATASET_LOC}/train\"\n",
    "VAL_DIR = f\"{DATASET_LOC}/valid\" if os.path.exists(f\"{DATASET_LOC}/valid\") else None\n",
    "\n",
    "print(f\"\\n‚úÖ Dataset downloaded successfully!\")\n",
    "print(f\"   Location: {DATASET_LOC}\")\n",
    "print(f\"   Train images: {len(list(Path(TRAIN_DIR).glob('**/*.*')))} files\")\n",
    "if VAL_DIR:\n",
    "    print(f\"   Valid images: {len(list(Path(VAL_DIR).glob('**/*.*')))} files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9393998e",
   "metadata": {},
   "source": [
    "## üß† Step 2: Define Model Architecture\n",
    "\n",
    "Vision Transformer with Skip Connections for better detail preservation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "678cbfe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbed(nn.Module):\n",
    "    \"\"\"Convert image to patches and embed them\"\"\"\n",
    "    def __init__(self, img_size=256, patch_size=16, in_c=3, embed_dim=768):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Sequential(\n",
    "            nn.Conv2d(in_c, embed_dim, kernel_size=patch_size, stride=patch_size),\n",
    "            Rearrange('b c h w -> b (h w) c')\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.proj(x)\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    \"\"\"Multi-head self-attention\"\"\"\n",
    "    def __init__(self, dim=768, heads=12, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.heads = heads\n",
    "        self.scale = (dim // heads) ** -0.5\n",
    "        self.qkv = nn.Linear(dim, dim * 3)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B, N, C = x.shape\n",
    "        qkv = self.qkv(x).reshape(B, N, 3, self.heads, C // self.heads).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
    "        return self.proj(self.drop(x))\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    \"\"\"Feed-forward network\"\"\"\n",
    "    def __init__(self, dim=768, hidden=3072, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(dim, hidden)\n",
    "        self.fc2 = nn.Linear(hidden, dim)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.drop(self.fc2(self.drop(F.gelu(self.fc1(x)))))\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\"Transformer block\"\"\"\n",
    "    def __init__(self, dim=768, heads=12, mlp_ratio=4.0, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(dim)\n",
    "        self.attn = Attention(dim, heads, dropout)\n",
    "        self.norm2 = nn.LayerNorm(dim)\n",
    "        self.mlp = MLP(dim, int(dim * mlp_ratio), dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.norm1(x))\n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "        return x\n",
    "\n",
    "class ViTRestorer(nn.Module):\n",
    "    \"\"\"\n",
    "    Vision Transformer for Image Restoration\n",
    "    WITH SKIP CONNECTIONS for better detail preservation\n",
    "    \"\"\"\n",
    "    def __init__(self, img_size=256, patch_size=16, in_c=3, out_c=3,\n",
    "                 embed_dim=768, depth=12, heads=12, mlp_ratio=4.0, use_skip=True):\n",
    "        super().__init__()\n",
    "        self.patch_embed = PatchEmbed(img_size, patch_size, in_c, embed_dim)\n",
    "        num_patches = (img_size // patch_size) ** 2\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches, embed_dim))\n",
    "        self.blocks = nn.ModuleList([Block(embed_dim, heads, mlp_ratio) for _ in range(depth)])\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        \n",
    "        # Decoder head\n",
    "        self.head = nn.Linear(embed_dim, patch_size * patch_size * out_c)\n",
    "        \n",
    "        # Skip connection fusion (IMPORTANT: preserves fine details from input)\n",
    "        self.use_skip = use_skip\n",
    "        if use_skip:\n",
    "            self.skip_fusion = nn.Conv2d(in_c + out_c, out_c, kernel_size=1)\n",
    "            nn.init.kaiming_normal_(self.skip_fusion.weight)\n",
    "            nn.init.constant_(self.skip_fusion.bias, 0)\n",
    "        \n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        nn.init.trunc_normal_(self.pos_embed, std=0.02)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Store input for skip connection\n",
    "        input_img = x\n",
    "        \n",
    "        # Encode: image patches + positional embedding\n",
    "        x = self.patch_embed(x) + self.pos_embed\n",
    "        \n",
    "        # Transform: apply transformer blocks\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        x = self.norm(x)\n",
    "        \n",
    "        # Decode: reconstruct image from patches\n",
    "        x = self.head(x)\n",
    "        h = w = self.img_size // self.patch_size\n",
    "        x = x.reshape(x.shape[0], h, w, self.patch_size, self.patch_size, 3)\n",
    "        x = x.permute(0, 5, 1, 3, 2, 4).reshape(x.shape[0], 3, self.img_size, self.img_size)\n",
    "        \n",
    "        # Apply skip connection (helps preserve input details)\n",
    "        if self.use_skip:\n",
    "            x = torch.cat([x, input_img], dim=1)  # Concatenate restored + input\n",
    "            x = self.skip_fusion(x)  # Fuse them intelligently\n",
    "        \n",
    "        return x\n",
    "\n",
    "total_params = sum(p.numel() for p in ViTRestorer().parameters())\n",
    "print(f\"‚úì Model architecture defined: {total_params:,} parameters\")\n",
    "print(f\"‚úì Skip connections: ENABLED (better detail preservation)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4452d2",
   "metadata": {},
   "source": [
    "## üìä Step 3: Define Loss Function\n",
    "\n",
    "Combined loss using:\n",
    "- **L1 Loss**: Pixel-level accuracy\n",
    "- **Perceptual Loss (LPIPS)**: Visual quality and texture preservation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff686b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CombinedLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Combined loss for better restoration:\n",
    "    - L1 loss for pixel-level accuracy\n",
    "    - Perceptual loss (LPIPS) for visual quality\n",
    "    \"\"\"\n",
    "    def __init__(self, l1_weight=1.0, perceptual_weight=0.1):\n",
    "        super().__init__()\n",
    "        self.l1_weight = l1_weight\n",
    "        self.perceptual_weight = perceptual_weight\n",
    "        \n",
    "        # Initialize LPIPS (perceptual loss)\n",
    "        try:\n",
    "            self.lpips_fn = lpips.LPIPS(net='alex')\n",
    "            self.use_perceptual = True\n",
    "            print(\"‚úì LPIPS perceptual loss enabled\")\n",
    "        except:\n",
    "            self.use_perceptual = False\n",
    "            print(\"‚ö†Ô∏è  LPIPS unavailable, using L1 only\")\n",
    "    \n",
    "    def forward(self, pred, target):\n",
    "        # L1 loss\n",
    "        l1_loss = F.l1_loss(pred, target)\n",
    "        \n",
    "        # Perceptual loss\n",
    "        if self.use_perceptual:\n",
    "            try:\n",
    "                # LPIPS expects inputs in [-1, 1] range\n",
    "                pred_scaled = pred * 2 - 1\n",
    "                target_scaled = target * 2 - 1\n",
    "                perceptual_loss = self.lpips_fn(pred_scaled, target_scaled).mean()\n",
    "                \n",
    "                total_loss = self.l1_weight * l1_loss + self.perceptual_weight * perceptual_loss\n",
    "                return total_loss\n",
    "            except:\n",
    "                return l1_loss\n",
    "        else:\n",
    "            return l1_loss\n",
    "\n",
    "print(\"‚úì Loss function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d7bacf",
   "metadata": {},
   "source": [
    "## üñºÔ∏è Step 4: Create Dataset with Synthetic Degradation\n",
    "\n",
    "**IMPORTANT: This is how paired training works!**\n",
    "\n",
    "For each clean image:\n",
    "1. Load the clean image (target)\n",
    "2. Apply synthetic degradation (creates the degraded input)\n",
    "3. Model learns to map: degraded ‚Üí clean\n",
    "\n",
    "Degradation techniques simulate real manuscript damage:\n",
    "- Noise (scanning artifacts, paper texture)\n",
    "- Blur (age, poor focus)\n",
    "- Fading (ink degradation)\n",
    "- Color shifting (paper yellowing)\n",
    "- Compression artifacts (poor digitization)\n",
    "- Stains and spots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c173674",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ManuscriptDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for manuscript restoration with synthetic degradation\n",
    "    \n",
    "    For each image:\n",
    "    1. Load clean image (target)\n",
    "    2. Create degraded version (input) using synthetic degradation\n",
    "    3. Return pair: {degraded, clean}\n",
    "    \"\"\"\n",
    "    def __init__(self, data_dir, img_size=256, mode='train', augment=True):\n",
    "        self.data_dir = Path(data_dir)\n",
    "        self.img_size = img_size\n",
    "        self.mode = mode\n",
    "        self.augment = augment and mode == 'train'\n",
    "        \n",
    "        # Find all images\n",
    "        exts = ['.jpg', '.jpeg', '.png', '.tif', '.tiff', '.bmp']\n",
    "        self.images = []\n",
    "        for ext in exts:\n",
    "            self.images.extend(list(self.data_dir.glob(f'**/*{ext}')))\n",
    "            self.images.extend(list(self.data_dir.glob(f'**/*{ext.upper()}')))\n",
    "        self.images = sorted(set(self.images))\n",
    "        print(f\"{mode}: {len(self.images)} images found\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def _degrade(self, img):\n",
    "        \"\"\"\n",
    "        Apply realistic manuscript degradation\n",
    "        This simulates real-world manuscript damage\n",
    "        \"\"\"\n",
    "        img = img.astype(np.float32) / 255.0\n",
    "        \n",
    "        # 1. Additive noise (scanning noise, paper texture)\n",
    "        if random.random() > 0.2:\n",
    "            noise_level = random.uniform(0.02, 0.10)\n",
    "            img += np.random.normal(0, noise_level, img.shape)\n",
    "        \n",
    "        # 2. Gaussian blur (focus issues, age blur)\n",
    "        if random.random() > 0.2:\n",
    "            kernel_size = random.choice([3, 5, 7])\n",
    "            img = cv2.GaussianBlur(img, (kernel_size, kernel_size), 0)\n",
    "        \n",
    "        # 3. Contrast/brightness degradation (faded ink)\n",
    "        if random.random() > 0.3:\n",
    "            contrast = random.uniform(0.5, 0.9)\n",
    "            brightness = random.uniform(0.05, 0.20)\n",
    "            img = contrast * img + brightness\n",
    "        \n",
    "        # 4. Aging tint (yellowing paper)\n",
    "        if random.random() > 0.3:\n",
    "            tint = np.array([1.0, random.uniform(0.90, 0.98), random.uniform(0.75, 0.90)])\n",
    "            img *= tint\n",
    "        \n",
    "        # 5. Salt and pepper noise (stains, spots)\n",
    "        if random.random() > 0.5:\n",
    "            noise_ratio = random.uniform(0.001, 0.01)\n",
    "            mask = np.random.random(img.shape[:2]) < noise_ratio\n",
    "            img[mask] = np.random.choice([0, 1], size=(mask.sum(), 3))\n",
    "        \n",
    "        # 6. JPEG compression artifacts (poor digitization)\n",
    "        if random.random() > 0.5:\n",
    "            img_uint8 = (np.clip(img, 0, 1) * 255).astype(np.uint8)\n",
    "            quality = random.randint(60, 90)\n",
    "            encode_param = [int(cv2.IMWRITE_JPEG_QUALITY), quality]\n",
    "            _, enc = cv2.imencode('.jpg', img_uint8, encode_param)\n",
    "            img = cv2.imdecode(enc, 1).astype(np.float32) / 255.0\n",
    "        \n",
    "        return np.clip(img, 0, 1)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Load image\n",
    "        img_path = self.images[idx]\n",
    "        img = cv2.imread(str(img_path))\n",
    "        if img is None:\n",
    "            img = np.array(Image.open(img_path).convert('RGB'))\n",
    "        else:\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # PAIRED DATA CREATION:\n",
    "        # Clean image = target (what we want model to output)\n",
    "        clean = img.astype(np.float32) / 255.0\n",
    "        \n",
    "        # Degraded image = input (what we feed to model)\n",
    "        degraded = self._degrade(img.copy())\n",
    "        \n",
    "        # Data augmentation (applied to BOTH images to maintain pairing)\n",
    "        if self.augment:\n",
    "            # Random horizontal flip\n",
    "            if random.random() > 0.5:\n",
    "                degraded = np.fliplr(degraded).copy()\n",
    "                clean = np.fliplr(clean).copy()\n",
    "            # Random rotation\n",
    "            if random.random() > 0.5:\n",
    "                angle = random.choice([90, 180, 270])\n",
    "                degraded = np.rot90(degraded, k=angle//90).copy()\n",
    "                clean = np.rot90(clean, k=angle//90).copy()\n",
    "        \n",
    "        # Resize to model input size\n",
    "        degraded = cv2.resize(degraded, (self.img_size, self.img_size))\n",
    "        clean = cv2.resize(clean, (self.img_size, self.img_size))\n",
    "        \n",
    "        # Convert to PyTorch tensors\n",
    "        degraded = torch.from_numpy(degraded).permute(2, 0, 1).float()\n",
    "        clean = torch.from_numpy(clean).permute(2, 0, 1).float()\n",
    "        \n",
    "        return {'degraded': degraded, 'clean': clean}\n",
    "\n",
    "print(\"‚úì Dataset class defined with synthetic degradation pipeline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a416afc",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Step 5: Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66a9abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training hyperparameters\n",
    "IMG_SIZE = 256\n",
    "BATCH_SIZE = 16\n",
    "NUM_EPOCHS = 100\n",
    "LEARNING_RATE = 1e-4\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(f\"Training Configuration:\")\n",
    "print(f\"  Device: {DEVICE}\")\n",
    "print(f\"  Image size: {IMG_SIZE}x{IMG_SIZE}\")\n",
    "print(f\"  Batch size: {BATCH_SIZE}\")\n",
    "print(f\"  Epochs: {NUM_EPOCHS}\")\n",
    "print(f\"  Learning rate: {LEARNING_RATE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "231ba78f",
   "metadata": {},
   "source": [
    "## üì¶ Step 6: Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f186cfc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training dataset\n",
    "train_dataset = ManuscriptDataset(TRAIN_DIR, IMG_SIZE, 'train', augment=True)\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=True, \n",
    "    num_workers=2, \n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "# Create validation dataset (if available)\n",
    "val_loader = None\n",
    "if VAL_DIR:\n",
    "    val_dataset = ManuscriptDataset(VAL_DIR, IMG_SIZE, 'val', augment=False)\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset, \n",
    "        batch_size=BATCH_SIZE, \n",
    "        shuffle=False, \n",
    "        num_workers=2, \n",
    "        pin_memory=True\n",
    "    )\n",
    "\n",
    "print(f\"\\nDataset Summary:\")\n",
    "print(f\"  Train batches: {len(train_loader)}\")\n",
    "print(f\"  Train images: {len(train_dataset)}\")\n",
    "if val_loader:\n",
    "    print(f\"  Val batches: {len(val_loader)}\")\n",
    "    print(f\"  Val images: {len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550365f6",
   "metadata": {},
   "source": [
    "## üöÄ Step 7: Create Model & Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ec7adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model with skip connections enabled\n",
    "model = ViTRestorer(\n",
    "    img_size=IMG_SIZE,\n",
    "    embed_dim=768,\n",
    "    depth=12,\n",
    "    heads=12,\n",
    "    use_skip=True  # IMPORTANT: Enable skip connections\n",
    ").to(DEVICE)\n",
    "\n",
    "# Loss function (L1 + Perceptual)\n",
    "criterion = CombinedLoss(l1_weight=1.0, perceptual_weight=0.1)\n",
    "criterion = criterion.to(DEVICE)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=0.01)\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, NUM_EPOCHS, eta_min=1e-6)\n",
    "\n",
    "# Create checkpoint directory\n",
    "os.makedirs('/kaggle/working/checkpoints', exist_ok=True)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"\\nModel Summary:\")\n",
    "print(f\"  Total parameters: {total_params:,}\")\n",
    "print(f\"  Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"  Model size: ~{total_params * 4 / 1e6:.1f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5592b422",
   "metadata": {},
   "source": [
    "## üèãÔ∏è Step 8: Training Loop\n",
    "\n",
    "Training process:\n",
    "1. **Load batch** of degraded/clean pairs\n",
    "2. **Forward pass**: degraded ‚Üí model ‚Üí restored\n",
    "3. **Calculate loss**: compare restored vs clean\n",
    "4. **Backward pass**: update model weights\n",
    "5. **Validate**: check performance on validation set\n",
    "6. **Save best model**: based on PSNR metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec066d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_psnr(pred, target):\n",
    "    \"\"\"Calculate PSNR metric\"\"\"\n",
    "    pred_np = pred.detach().cpu().numpy()\n",
    "    target_np = target.detach().cpu().numpy()\n",
    "    psnrs = []\n",
    "    for i in range(pred_np.shape[0]):\n",
    "        psnr = peak_signal_noise_ratio(target_np[i], pred_np[i], data_range=1.0)\n",
    "        psnrs.append(psnr)\n",
    "    return np.mean(psnrs)\n",
    "\n",
    "# Training state\n",
    "best_psnr = 0.0\n",
    "training_history = []\n",
    "\n",
    "print(\"üèãÔ∏è Starting training...\\n\")\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    # ========== TRAINING ==========\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    train_psnr = 0.0\n",
    "    \n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS}\")\n",
    "    for batch_idx, batch in enumerate(pbar):\n",
    "        # Get degraded input and clean target\n",
    "        degraded = batch['degraded'].to(DEVICE)\n",
    "        clean = batch['clean'].to(DEVICE)\n",
    "        \n",
    "        # Forward pass: degraded ‚Üí model ‚Üí restored\n",
    "        optimizer.zero_grad()\n",
    "        restored = model(degraded)\n",
    "        \n",
    "        # Calculate loss: how different is restored from clean?\n",
    "        loss = criterion(restored, clean)\n",
    "        \n",
    "        # Backward pass: update weights\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Track metrics\n",
    "        train_loss += loss.item()\n",
    "        with torch.no_grad():\n",
    "            train_psnr += calc_psnr(restored, clean)\n",
    "        \n",
    "        # Update progress bar\n",
    "        pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "    \n",
    "    # Average metrics\n",
    "    train_loss /= len(train_loader)\n",
    "    train_psnr /= len(train_loader)\n",
    "    \n",
    "    # Update learning rate\n",
    "    scheduler.step()\n",
    "    \n",
    "    # ========== VALIDATION ==========\n",
    "    if val_loader and (epoch + 1) % 5 == 0:\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_psnr = 0.0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                degraded = batch['degraded'].to(DEVICE)\n",
    "                clean = batch['clean'].to(DEVICE)\n",
    "                \n",
    "                restored = model(degraded)\n",
    "                val_loss += criterion(restored, clean).item()\n",
    "                val_psnr += calc_psnr(restored, clean)\n",
    "        \n",
    "        val_loss /= len(val_loader)\n",
    "        val_psnr /= len(val_loader)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{NUM_EPOCHS}: \"\n",
    "              f\"Train Loss={train_loss:.4f}, Train PSNR={train_psnr:.2f} | \"\n",
    "              f\"Val Loss={val_loss:.4f}, Val PSNR={val_psnr:.2f}\")\n",
    "        \n",
    "        # Save best model\n",
    "        if val_psnr > best_psnr:\n",
    "            best_psnr = val_psnr\n",
    "            torch.save({\n",
    "                'epoch': epoch + 1,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'best_val_psnr': best_psnr,\n",
    "                'config': {\n",
    "                    'img_size': IMG_SIZE,\n",
    "                    'embed_dim': 768,\n",
    "                    'depth': 12,\n",
    "                    'heads': 12,\n",
    "                    'use_skip': True\n",
    "                }\n",
    "            }, '/kaggle/working/checkpoints/best_psnr.pth')\n",
    "            print(f\"  ‚úì New best PSNR: {best_psnr:.2f} dB (model saved!)\")\n",
    "        \n",
    "        # Save history\n",
    "        training_history.append({\n",
    "            'epoch': epoch + 1,\n",
    "            'train_loss': train_loss,\n",
    "            'train_psnr': train_psnr,\n",
    "            'val_loss': val_loss,\n",
    "            'val_psnr': val_psnr\n",
    "        })\n",
    "    else:\n",
    "        print(f\"Epoch {epoch+1}/{NUM_EPOCHS}: \"\n",
    "              f\"Train Loss={train_loss:.4f}, Train PSNR={train_psnr:.2f}\")\n",
    "    \n",
    "    # Save periodic checkpoint\n",
    "    if (epoch + 1) % 20 == 0:\n",
    "        torch.save(model.state_dict(), f'/kaggle/working/checkpoints/epoch_{epoch+1}.pth')\n",
    "        print(f\"  ‚úì Checkpoint saved: epoch_{epoch+1}.pth\")\n",
    "\n",
    "# Save final model\n",
    "torch.save(model.state_dict(), '/kaggle/working/checkpoints/final.pth')\n",
    "print(f\"\\n‚úÖ Training complete!\")\n",
    "print(f\"   Best validation PSNR: {best_psnr:.2f} dB\")\n",
    "\n",
    "# Save training history\n",
    "with open('/kaggle/working/training_history.json', 'w') as f:\n",
    "    json.dump(training_history, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a1fc046",
   "metadata": {},
   "source": [
    "## üß™ Step 9: Test Model on Sample Images\n",
    "\n",
    "Load best model and test on samples to visualize results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4d55cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üß™ Testing trained model on samples...\\n\")\n",
    "\n",
    "# Load best model\n",
    "if os.path.exists('/kaggle/working/checkpoints/best_psnr.pth'):\n",
    "    checkpoint = torch.load('/kaggle/working/checkpoints/best_psnr.pth')\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    print(f\"‚úì Loaded best model (Epoch {checkpoint['epoch']}, PSNR: {checkpoint['best_val_psnr']:.2f} dB)\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Using final model (best_psnr.pth not found)\")\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# Create test results directory\n",
    "os.makedirs('/kaggle/working/test_results', exist_ok=True)\n",
    "\n",
    "# Get sample images\n",
    "sample_images = list(Path(TRAIN_DIR).glob('*.jpg'))[:5]\n",
    "if not sample_images:\n",
    "    sample_images = list(Path(TRAIN_DIR).glob('**/*.jpg'))[:5]\n",
    "\n",
    "print(f\"Testing on {len(sample_images)} sample images...\\n\")\n",
    "\n",
    "test_results = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for idx, img_path in enumerate(sample_images):\n",
    "        # Load image\n",
    "        img = cv2.imread(str(img_path))\n",
    "        if img is None:\n",
    "            img = np.array(Image.open(img_path).convert('RGB'))\n",
    "        else:\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Create clean version (target)\n",
    "        clean_img = img.astype(np.float32) / 255.0\n",
    "        \n",
    "        # Create degraded version (input) using same degradation as training\n",
    "        degraded_img = clean_img.copy()\n",
    "        degraded_img += np.random.normal(0, 0.05, degraded_img.shape)\n",
    "        degraded_img = cv2.GaussianBlur(degraded_img, (5, 5), 0)\n",
    "        degraded_img = 0.7 * degraded_img + 0.1\n",
    "        degraded_img *= np.array([1.0, 0.95, 0.8])\n",
    "        degraded_img = np.clip(degraded_img, 0, 1)\n",
    "        \n",
    "        # Resize and convert to tensor\n",
    "        clean_resized = cv2.resize(clean_img, (IMG_SIZE, IMG_SIZE))\n",
    "        degraded_resized = cv2.resize(degraded_img, (IMG_SIZE, IMG_SIZE))\n",
    "        \n",
    "        degraded_tensor = torch.from_numpy(degraded_resized).permute(2, 0, 1).unsqueeze(0).float().to(DEVICE)\n",
    "        \n",
    "        # Restore using trained model\n",
    "        restored_tensor = model(degraded_tensor)\n",
    "        \n",
    "        # Convert back to numpy\n",
    "        restored_img = restored_tensor.squeeze(0).permute(1, 2, 0).cpu().numpy()\n",
    "        restored_img = np.clip(restored_img, 0, 1)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        psnr = peak_signal_noise_ratio(clean_resized, restored_img, data_range=1.0)\n",
    "        ssim = structural_similarity(clean_resized, restored_img, multichannel=True, channel_axis=2, data_range=1.0)\n",
    "        \n",
    "        test_results.append({'sample': idx+1, 'psnr': psnr, 'ssim': ssim})\n",
    "        print(f\"  Sample {idx+1}: PSNR={psnr:.2f} dB, SSIM={ssim:.4f}\")\n",
    "        \n",
    "        # Save comparison (degraded | restored | clean)\n",
    "        comparison = np.hstack([\n",
    "            (degraded_resized * 255).astype(np.uint8),\n",
    "            (restored_img * 255).astype(np.uint8),\n",
    "            (clean_resized * 255).astype(np.uint8)\n",
    "        ])\n",
    "        \n",
    "        cv2.imwrite(\n",
    "            f'/kaggle/working/test_results/sample_{idx+1}_comparison.jpg',\n",
    "            cv2.cvtColor(comparison, cv2.COLOR_RGB2BGR)\n",
    "        )\n",
    "\n",
    "print(f\"\\n‚úÖ Test results saved to /kaggle/working/test_results/\")\n",
    "print(f\"   Average PSNR: {np.mean([r['psnr'] for r in test_results]):.2f} dB\")\n",
    "print(f\"   Average SSIM: {np.mean([r['ssim'] for r in test_results]):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a69ff4",
   "metadata": {},
   "source": [
    "## üìä Step 10: Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c429accc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display restoration examples\n",
    "fig, axes = plt.subplots(min(3, len(sample_images)), 3, figsize=(15, 5*min(3, len(sample_images))))\n",
    "if len(sample_images) == 1:\n",
    "    axes = axes.reshape(1, -1)\n",
    "\n",
    "for idx in range(min(3, len(sample_images))):\n",
    "    img_path = f'/kaggle/working/test_results/sample_{idx+1}_comparison.jpg'\n",
    "    if os.path.exists(img_path):\n",
    "        comparison = cv2.imread(img_path)\n",
    "        comparison = cv2.cvtColor(comparison, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Split into three parts\n",
    "        h, w = comparison.shape[:2]\n",
    "        w_third = w // 3\n",
    "        \n",
    "        degraded = comparison[:, :w_third]\n",
    "        restored = comparison[:, w_third:2*w_third]\n",
    "        clean = comparison[:, 2*w_third:]\n",
    "        \n",
    "        axes[idx, 0].imshow(degraded)\n",
    "        axes[idx, 0].set_title('Degraded Input', fontsize=12)\n",
    "        axes[idx, 0].axis('off')\n",
    "        \n",
    "        axes[idx, 1].imshow(restored)\n",
    "        axes[idx, 1].set_title('Restored Output', fontsize=12, color='green')\n",
    "        axes[idx, 1].axis('off')\n",
    "        \n",
    "        axes[idx, 2].imshow(clean)\n",
    "        axes[idx, 2].set_title('Clean Target', fontsize=12)\n",
    "        axes[idx, 2].axis('off')\n",
    "\n",
    "plt.suptitle('Manuscript Restoration Results', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig('/kaggle/working/restoration_examples.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Visualization saved to /kaggle/working/restoration_examples.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b80030",
   "metadata": {},
   "source": [
    "## üíæ Step 11: Save Models for Download\n",
    "\n",
    "Save models in formats compatible with your local project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2fc961a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final model (state dict only - lightweight)\n",
    "torch.save(model.state_dict(), '/kaggle/working/final.pth')\n",
    "print(\"‚úì Saved: final.pth (state dict only, ~330 MB)\")\n",
    "\n",
    "# Save complete checkpoint (with optimizer state - for resuming training)\n",
    "torch.save({\n",
    "    'epoch': NUM_EPOCHS,\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'scheduler_state_dict': scheduler.state_dict(),\n",
    "    'best_psnr': best_psnr,\n",
    "    'training_history': training_history,\n",
    "    'config': {\n",
    "        'img_size': IMG_SIZE,\n",
    "        'embed_dim': 768,\n",
    "        'depth': 12,\n",
    "        'heads': 12,\n",
    "        'mlp_ratio': 4.0,\n",
    "        'use_skip': True\n",
    "    }\n",
    "}, '/kaggle/working/desti.pth')\n",
    "print(\"‚úì Saved: desti.pth (complete checkpoint with training state, ~990 MB)\")\n",
    "\n",
    "print(f\"\\n‚úÖ All models saved in /kaggle/working/\")\n",
    "print(f\"\\nüì• To download:\")\n",
    "print(f\"   1. Click 'Output' tab on the right ‚Üí\")\n",
    "print(f\"   2. Download these files:\")\n",
    "print(f\"      ‚Ä¢ best_psnr.pth (recommended for inference)\")\n",
    "print(f\"      ‚Ä¢ final.pth (final epoch)\")\n",
    "print(f\"      ‚Ä¢ desti.pth (complete checkpoint)\")\n",
    "print(f\"      ‚Ä¢ restoration_examples.png (visual results)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "583492df",
   "metadata": {},
   "source": [
    "## üöÄ Step 12: Use Trained Models Locally\n",
    "\n",
    "### üì• After downloading models:\n",
    "\n",
    "```bash\n",
    "# 1. Place models in your project\n",
    "mkdir -p /home/bagesh/EL-project/checkpoints/kaggle\n",
    "cp ~/Downloads/final.pth /home/bagesh/EL-project/checkpoints/kaggle/\n",
    "cp ~/Downloads/desti.pth /home/bagesh/EL-project/checkpoints/kaggle/\n",
    "\n",
    "# 2. Test the model\n",
    "cd /home/bagesh/EL-project\n",
    "source activate_venv.sh\n",
    "\n",
    "# 3. Run inference on test images\n",
    "python inference.py \\\n",
    "    --checkpoint checkpoints/kaggle/final.pth \\\n",
    "    --input data/raw/test/ \\\n",
    "    --output output/kaggle_results\n",
    "\n",
    "# 4. Or use the full pipeline\n",
    "python main.py \\\n",
    "    --image_path data/raw/test/test_0001.jpg \\\n",
    "    --restoration_model checkpoints/kaggle/final.pth\n",
    "\n",
    "# 5. Or run the web UI\n",
    "streamlit run app.py\n",
    "```\n",
    "\n",
    "### üìä Training Summary\n",
    "- **Best PSNR**: {best_psnr:.2f} dB\n",
    "- **Total Epochs**: {NUM_EPOCHS}\n",
    "- **Model Parameters**: 86.4M\n",
    "- **Training Time**: ~5 hours on GPU T4 x2\n",
    "\n",
    "### üéØ Model Architecture\n",
    "- **Type**: Vision Transformer (ViT)\n",
    "- **Input**: 256x256 degraded manuscript image\n",
    "- **Output**: 256x256 restored image\n",
    "- **Skip Connections**: Enabled (preserves fine details)\n",
    "- **Loss**: L1 + Perceptual (LPIPS)\n",
    "\n",
    "---\n",
    "\n",
    "üéâ **Training Complete!** Your model is ready to restore ancient manuscripts!"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
