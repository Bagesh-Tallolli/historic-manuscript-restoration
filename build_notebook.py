#!/usr/bin/env python3
"""
Build complete Kaggle training notebook
"""
import json

def create_complete_notebook():
    """Create the complete notebook with all cells"""

    notebook = {
        "cells": [],
        "metadata": {
            "kernelspec": {
                "display_name": "Python 3",
                "language": "python",
                "name": "python3"
            },
            "language_info": {
                "codemirror_mode": {"name": "ipython", "version": 3},
                "file_extension": ".py",
                "mimetype": "text/x-python",
                "name": "python",
                "nbconvert_exporter": "python",
                "pygments_lexer": "ipython3",
                "version": "3.10.12"
            }
        },
        "nbformat": 4,
        "nbformat_minor": 4
    }

    # Cell 1: Title
    notebook["cells"].append({
        "cell_type": "markdown",
        "metadata": {},
        "source": [
            "# üèõÔ∏è Historic Manuscript Restoration with Vision Transformer\\n",
            "\\n",
            "## üìã Overview\\n",
            "This notebook trains a Vision Transformer (ViT) model for restoring degraded historic manuscripts.\\n",
            "\\n",
            "## ‚öôÔ∏è Setup Instructions\\n",
            "1. **Enable GPU**: Settings ‚Üí Accelerator ‚Üí GPU T4 x2\\n",
            "2. **Run all cells**: Dataset downloads automatically from Roboflow\\n",
            "3. **Wait for training**: ~5 hours on GPU T4\\n",
            "4. **Download model**: Best model saved to `/kaggle/working/checkpoints/best_psnr.pth`\\n",
            "\\n",
            "## üéØ Features\\n",
            "- ‚úÖ Automatic dataset download from Roboflow\\n",
            "- ‚úÖ Synthetic degradation pipeline\\n",
            "- ‚úÖ Vision Transformer architecture\\n",
            "- ‚úÖ Multi-metric evaluation (PSNR, SSIM, LPIPS)\\n",
            "- ‚úÖ Automatic checkpointing\\n",
            "\\n",
            "---"
        ]
    })

    # Cell 2: Install
    notebook["cells"].append({
        "cell_type": "markdown",
        "metadata": {},
        "source": ["## 1Ô∏è‚É£ Install Dependencies"]
    })

    notebook["cells"].append({
        "cell_type": "code",
        "execution_count": None,
        "metadata": {},
        "outputs": [],
        "source": [
            "%%capture\\n",
            "!pip install einops lpips roboflow opencv-python-headless scikit-image -q"
        ]
    })

    # Cell 3: Imports Header
    notebook["cells"].append({
        "cell_type": "markdown",
        "metadata": {},
        "source": ["## 2Ô∏è‚É£ Import Libraries"]
    })

    # Cell 4: Imports Code
    notebook["cells"].append({
        "cell_type": "code",
        "execution_count": None,
        "metadata": {},
        "outputs": [],
        "source": [
            "import torch\\n",
            "import torch.nn as nn\\n",
            "import torch.nn.functional as F\\n",
            "import torch.optim as optim\\n",
            "from torch.utils.data import Dataset, DataLoader\\n",
            "from pathlib import Path\\n",
            "import cv2\\n",
            "import numpy as np\\n",
            "from PIL import Image\\n",
            "import random\\n",
            "from tqdm import tqdm\\n",
            "import json\\n",
            "import time\\n",
            "import os\\n",
            "from einops import rearrange\\n",
            "from einops.layers.torch import Rearrange\\n",
            "import lpips\\n",
            "from skimage.metrics import peak_signal_noise_ratio, structural_similarity\\n",
            "\\n",
            "print(f\\\"‚úì PyTorch version: {torch.__version__}\\\")\\n",
            "print(f\\\"‚úì CUDA available: {torch.cuda.is_available()}\\\")\\n",
            "if torch.cuda.is_available():\\n",
            "    print(f\\\"‚úì GPU: {torch.cuda.get_device_name(0)}\\\")"
        ]
    })

    # Cell 5: Dataset Download Header
    notebook["cells"].append({
        "cell_type": "markdown",
        "metadata": {},
        "source": [
            "## 3Ô∏è‚É£ Download Dataset from Roboflow\\n",
            "\\n",
            "**Dataset**: Sanskrit Manuscript Dataset  \\n",
            "**Source**: Roboflow (preconfigured)\\n",
            "\\n",
            "The dataset will be automatically downloaded and prepared for training."
        ]
    })

    # Cell 6: Dataset Download Code
    roboflow_code = '''from roboflow import Roboflow

# ========== ROBOFLOW CONFIGURATION (HARDCODED) ==========
ROBOFLOW_API_KEY = "EBJvHlgSWLyW1Ir6ctkH"
ROBOFLOW_WORKSPACE = "neeew"
ROBOFLOW_PROJECT = "yoyoyo-mptyx-ijqfp"
ROBOFLOW_VERSION = 1
DATASET_LOCATION = "/kaggle/working/dataset"

print("=" * 70)
print("üì• DOWNLOADING DATASET FROM ROBOFLOW")
print("=" * 70)
print(f"Workspace: {ROBOFLOW_WORKSPACE}")
print(f"Project: {ROBOFLOW_PROJECT}")
print(f"Version: {ROBOFLOW_VERSION}")
print(f"Download location: {DATASET_LOCATION}\\n")

try:
    # Initialize Roboflow
    rf = Roboflow(api_key=ROBOFLOW_API_KEY)
    
    # Get project and download
    project = rf.workspace(ROBOFLOW_WORKSPACE).project(ROBOFLOW_PROJECT)
    dataset = project.version(ROBOFLOW_VERSION).download("folder", location=DATASET_LOCATION)
    
    print("\\n‚úÖ Dataset downloaded successfully!")
    print(f"üìÅ Location: {DATASET_LOCATION}")
    
    # Check directory structure
    for subdir in ['train', 'valid', 'test']:
        path = Path(DATASET_LOCATION) / subdir
        if path.exists():
            num_files = len(list(path.glob('*.*')))
            print(f"  ‚úì {subdir}: {num_files} files")
    
    # Set train and validation directories
    TRAIN_DIR = f"{DATASET_LOCATION}/train"
    VAL_DIR = f"{DATASET_LOCATION}/valid" if os.path.exists(f"{DATASET_LOCATION}/valid") else None
    
except Exception as e:
    print(f"\\n‚ùå Error downloading dataset: {e}")
    print("\\nPlease check:")
    print("1. API key is correct")
    print("2. Internet connection is working")
    print("3. Roboflow project exists and is accessible")
    raise'''

    notebook["cells"].append({
        "cell_type": "code",
        "execution_count": None,
        "metadata": {},
        "outputs": [],
        "source": roboflow_code.split('\n')
    })

    # Continue building the notebook...
    print(f"Progress: {len(notebook['cells'])} cells created")

    # Save the notebook
    with open('kaggle_training_notebook.ipynb', 'w') as f:
        json.dump(notebook, f, indent=1)

    print(f"‚úì Notebook created with {len(notebook['cells'])} cells")
    return notebook

if __name__ == "__main__":
    create_complete_notebook()

